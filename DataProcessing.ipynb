{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94e03336",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, csv\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib widget\n",
    "from matplotlib.pylab import size\n",
    "import sys\n",
    "sys.path.append(r\"D:\\Files\\2025_Y4_S2\\AMME5710\\Major\")  # add the folder to the Python path\n",
    "import pandas as pd\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import AutoGate\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7715cd3",
   "metadata": {},
   "source": [
    "### Graphical based preprocessing and feature extraction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fd52173",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gray_world_white_balance(img, mask=None):\n",
    "    \"\"\"\n",
    "    Gray-world white balance.\n",
    "    If a mask is provided, compute averages only within the mask region\n",
    "    (useful for hand region to avoid background color bias).\n",
    "    \"\"\"\n",
    "    img_f = img.astype(np.float32)\n",
    "    if mask is not None:\n",
    "        mask_u8 = (mask > 0).astype(np.uint8)\n",
    "        avg_b = cv2.mean(img_f[:,:,0], mask_u8)[0]\n",
    "        avg_g = cv2.mean(img_f[:,:,1], mask_u8)[0]\n",
    "        avg_r = cv2.mean(img_f[:,:,2], mask_u8)[0]\n",
    "    else:\n",
    "        avg_b, avg_g, avg_r = cv2.mean(img_f)[:3]\n",
    "\n",
    "    avg_gray = (avg_b + avg_g + avg_r) / 3.0\n",
    "    scale = [\n",
    "        avg_gray / (avg_b + 1e-6),\n",
    "        avg_gray / (avg_g + 1e-6),\n",
    "        avg_gray / (avg_r + 1e-6)\n",
    "    ]\n",
    "\n",
    "    img_f[:,:,0] *= scale[0]\n",
    "    img_f[:,:,1] *= scale[1]\n",
    "    img_f[:,:,2] *= scale[2]\n",
    "    return np.clip(img_f, 0, 255).astype(np.uint8)\n",
    "\n",
    "\n",
    "def adjust_gamma(img, gamma=1.0):\n",
    "    \"\"\"\n",
    "    Apply gamma correction.\n",
    "    Note: gamma > 1 brightens the image; gamma < 1 darkens it.\n",
    "    \"\"\"\n",
    "    invGamma = 1.0 / max(gamma, 1e-6)\n",
    "    table = ((np.arange(256) / 255.0) ** invGamma * 255.0).astype(np.uint8)\n",
    "    return cv2.LUT(img, table)\n",
    "\n",
    "\n",
    "def clahe_on_l_lab(img_bgr, clip=2.0, tile=(8,8), mask=None):\n",
    "    \"\"\"\n",
    "    Apply CLAHE (local contrast enhancement) on the L channel in Lab color space.\n",
    "    If a mask is provided, only enhance the masked region (e.g., hand area).\n",
    "    \"\"\"\n",
    "    lab = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2LAB)\n",
    "    L, a, b = cv2.split(lab)\n",
    "    clahe = cv2.createCLAHE(clipLimit=float(clip), tileGridSize=tuple(tile))\n",
    "\n",
    "    if mask is None:\n",
    "        Lc = clahe.apply(L)\n",
    "    else:\n",
    "        Lc_all = clahe.apply(L)\n",
    "        Lc = L.copy()\n",
    "        Lc[mask > 0] = Lc_all[mask > 0]\n",
    "\n",
    "    merged = cv2.merge([Lc, a, b])\n",
    "    return cv2.cvtColor(merged, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "\n",
    "def denoise(img):\n",
    "    \"\"\"Light denoising using median filter and Non-local Means.\"\"\"\n",
    "    img = cv2.medianBlur(img, 3)\n",
    "    return img\n",
    "\n",
    "\n",
    "def normalize_hand_image(\n",
    "    img_bgr,\n",
    "    mask=None,              # optional binary mask for hand region\n",
    "    use_hist_eq=False,      # optional global histogram equalization\n",
    "    use_clahe=True,         # recommended for local contrast enhancement\n",
    "    clahe_clip=2.0,\n",
    "    clahe_tile=(12,12),\n",
    "    auto_gamma=True,\n",
    "    denoise_after=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Unified image normalization pipeline for hand gesture preprocessing.\n",
    "\n",
    "    Steps:\n",
    "      1. Gray-world white balance \n",
    "      2. Adaptive gamma correction \n",
    "      3. Optional global histogram equalization\n",
    "      4. CLAHE on L channel in Lab \n",
    "      5. Optional denoising \n",
    "\n",
    "    Notes:\n",
    "      - If both hist_eq and CLAHE are enabled, run CLAHE last.\n",
    "      - For dark images, gamma > 1.2 brightens them.\n",
    "      - For bright images, gamma < 1.0 darkens them slightly.\n",
    "      - If mask is provided, color and contrast are computed only on hand region.\n",
    "    \"\"\"\n",
    "    img = img_bgr.copy()\n",
    "\n",
    "    # 1) White balance (preferably on hand region only)\n",
    "    img = gray_world_white_balance(img, mask=mask)\n",
    "\n",
    "    # 2) Adaptive gamma correction\n",
    "    if auto_gamma:\n",
    "        if mask is not None:\n",
    "            mean_lum = float(cv2.mean(cv2.cvtColor(img, cv2.COLOR_BGR2GRAY), (mask>0).astype(np.uint8))[0])\n",
    "        else:\n",
    "            mean_lum = float(np.mean(cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)))\n",
    "\n",
    "        # empirical thresholds\n",
    "        if mean_lum < 100:\n",
    "            gamma = 1.25  # brighten dark images\n",
    "        elif mean_lum > 170:\n",
    "            gamma = 0.9   # slightly darken overly bright images\n",
    "        else:\n",
    "            gamma = 1.0   # keep as-is\n",
    "\n",
    "        img = adjust_gamma(img, gamma)\n",
    "\n",
    "    # 3) Optional global histogram equalization\n",
    "    if use_hist_eq:\n",
    "        ycrcb = cv2.cvtColor(img, cv2.COLOR_BGR2YCrCb)\n",
    "        y, cr, cb = cv2.split(ycrcb)\n",
    "        y_eq = cv2.equalizeHist(y)\n",
    "        img = cv2.cvtColor(cv2.merge([y_eq, cr, cb]), cv2.COLOR_YCrCb2BGR)\n",
    "\n",
    "    # 4) CLAHE on L channel\n",
    "    if use_clahe:\n",
    "        img = clahe_on_l_lab(img, clip=clahe_clip, tile=clahe_tile, mask=mask)\n",
    "        img = adjust_gamma(img, gamma=1.3)\n",
    "\n",
    "    # 5) Optional denoising\n",
    "    if denoise_after:\n",
    "        img = denoise(img)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63936d06",
   "metadata": {},
   "source": [
    "### Finger constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "139a6aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Landmark indices (MediaPipe)\n",
    "WRIST=0\n",
    "TH_1, TH_2, TH_3, TH_4 = 1,2,3,4\n",
    "IN_1, IN_2, IN_3, IN_4 = 5,6,7,8\n",
    "MI_1, MI_2, MI_3, MI_4 = 9,10,11,12\n",
    "RI_1, RI_2, RI_3, RI_4 = 13,14,15,16\n",
    "PI_1, PI_2, PI_3, PI_4 = 17,18,19,20\n",
    "\n",
    "# Chains (edges) for fingers\n",
    "FINGERS = [\n",
    "    [WRIST, TH_1, TH_2, TH_3, TH_4],\n",
    "    [WRIST, IN_1, IN_2, IN_3, IN_4],\n",
    "    [WRIST, MI_1, MI_2, MI_3, MI_4],\n",
    "    [WRIST, RI_1, RI_2, RI_3, RI_4],\n",
    "    [WRIST, PI_1, PI_2, PI_3, PI_4],\n",
    "]\n",
    "\n",
    "EDGES = []\n",
    "for chain in FINGERS:\n",
    "    EDGES += list(zip(chain[:-1], chain[1:]))\n",
    "\n",
    "HAND_CONNECTIONS = [\n",
    "    (0,1), (1,2), (2,3), (3,4),        # thumb\n",
    "    (0,5), (5,6), (6,7), (7,8),        # index\n",
    "    (0,9), (9,10), (10,11), (11,12),   # middle\n",
    "    (0,13), (13,14), (14,15), (15,16), # ring\n",
    "    (0,17), (17,18), (18,19), (19,20)  # pinky\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdea7da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: display image with correct color handling\n",
    "def show_image(title, img):\n",
    "    if img is None:\n",
    "        print(f\"[Warning] {title}: image is None\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(6,6))\n",
    "    # Convert BGR → RGB if color\n",
    "    if len(img.shape) == 3:\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        plt.imshow(img_rgb)\n",
    "    else:\n",
    "        plt.imshow(img, cmap='gray')\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# -------- Utilities --------\n",
    "def ensure_dir(p):\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "    return p\n",
    "\n",
    "def normalize_landmarks_xy(landmarks_xy):\n",
    "    \"\"\"Normalize 2D landmarks: make wrist=origin, scale by bbox diagonal for scale invariance.\"\"\"\n",
    "    L = landmarks_xy.astype(np.float32).copy()\n",
    "    wrist = L[0]\n",
    "    L -= wrist\n",
    "    minv, maxv = L.min(axis=0), L.max(axis=0)\n",
    "    diag = float(np.linalg.norm(maxv - minv) + 1e-6)\n",
    "    L /= diag\n",
    "    return L\n",
    "\n",
    "def landmarks_to_feature(landmarks_px_xyz):\n",
    "    \"\"\"(21,3): x_px,y_px,z_rel -> 63-D feature: [x',y',z] after 2D normalization.\"\"\"\n",
    "    xy_norm = normalize_landmarks_xy(landmarks_px_xyz[:, :2])\n",
    "    z = landmarks_px_xyz[:, 2:3]\n",
    "    feat = np.concatenate([xy_norm, z], axis=1).reshape(-1).astype(np.float32)\n",
    "    return feat  # (63,)\n",
    "\n",
    "def draw_landmarks_bgr(img_bgr, landmarks_px_xyz):\n",
    "    mp_draw = mp.solutions.drawing_utils\n",
    "    mp_style = mp.solutions.drawing_styles\n",
    "    mp_hands = mp.solutions.hands\n",
    "\n",
    "    # Rebuild normalized list for the drawer\n",
    "    h, w = img_bgr.shape[:2]\n",
    "    px, py, pz = (landmarks_px_xyz[:,0] / w), (landmarks_px_xyz[:,1] / h), landmarks_px_xyz[:,2]\n",
    "    nlms = landmark_pb2.NormalizedLandmarkList(\n",
    "        landmark=[\n",
    "            landmark_pb2.NormalizedLandmark(\n",
    "                x=float(x), y=float(y), z=float(z)\n",
    "            ) for x, y, z in zip(px, py, pz)\n",
    "        ]\n",
    "    )\n",
    "    out = img_bgr.copy()\n",
    "    mp_draw.draw_landmarks(\n",
    "        out, nlms, mp_hands.HAND_CONNECTIONS,\n",
    "        mp_style.get_default_hand_landmarks_style(),\n",
    "        mp_style.get_default_hand_connections_style()\n",
    "    )\n",
    "    return out\n",
    "\n",
    "\n",
    "def grabcut_mask_from_landmarks(img_bgr, landmarks_px_xyz, bbox_pad_ratio=0.15, iters=4):\n",
    "    \"\"\"Seed GrabCut using a small set of landmark points as sure-foreground; bbox ring as sure-background.\"\"\"\n",
    "    h, w = img_bgr.shape[:2]\n",
    "    # bbox from landmarks\n",
    "    x1 = int(np.clip(landmarks_px_xyz[:,0].min(), 0, w-1))\n",
    "    x2 = int(np.clip(landmarks_px_xyz[:,0].max(), 0, w-1))\n",
    "    y1 = int(np.clip(landmarks_px_xyz[:,1].min(), 0, h-1))\n",
    "    y2 = int(np.clip(landmarks_px_xyz[:,1].max(), 0, h-1))\n",
    "    pad = int(bbox_pad_ratio * max(x2 - x1, y2 - y1))\n",
    "    x1, y1 = max(0, x1 - pad), max(0, y1 - pad)\n",
    "    x2, y2 = min(w-1, x2 + pad), min(h-1, y2 + pad)\n",
    "\n",
    "    mask = np.full((h, w), cv2.GC_PR_BGD, np.uint8)\n",
    "    # sure foreground: wrist + MCPs (robust anchors)\n",
    "    for idx in [0,5,9,13,17]:\n",
    "        cx, cy = map(int, landmarks_px_xyz[idx, :2])\n",
    "        cv2.circle(mask, (cx, cy), 6, cv2.GC_FGD, -1)\n",
    "\n",
    "    # sure background: a ring outside bbox\n",
    "    ring = 6\n",
    "    cv2.rectangle(mask,\n",
    "                  (max(0, x1 - ring), max(0, y1 - ring)),\n",
    "                  (min(w-1, x2 + ring), min(h-1, y2 + ring)),\n",
    "                  cv2.GC_BGD, thickness=ring)\n",
    "\n",
    "    bgdModel = np.zeros((1,65), np.float64)\n",
    "    fgdModel = np.zeros((1,65), np.float64)\n",
    "    cv2.grabCut(img_bgr, mask, None, bgdModel, fgdModel, iters, cv2.GC_INIT_WITH_MASK)\n",
    "    hand_mask = np.where((mask==cv2.GC_FGD) | (mask==cv2.GC_PR_FGD), 255, 0).astype('uint8')\n",
    "\n",
    "    # small cleanup\n",
    "    hand_mask = cv2.morphologyEx(hand_mask, cv2.MORPH_CLOSE, np.ones((5,5), np.uint8))\n",
    "    return hand_mask\n",
    "\n",
    "def process_image_with_mediapipe(input_data, hands,\n",
    "                                 use_clahe=True, clahe_clip=2.0, clahe_tile=(12,12),\n",
    "                                 min_short_side=640,        # upscale small images\n",
    "                                 min_bbox_frac=0.03):       # reject tiny hands\n",
    "    \"\"\"\n",
    "    input_data: can be either\n",
    "        - str: path to an image file\n",
    "        - np.ndarray: preloaded BGR image (e.g., from cv2.imread or camera feed)\n",
    "    \"\"\"\n",
    "    # --- Handle input type ---\n",
    "    if isinstance(input_data, str):\n",
    "        img_bgr = cv2.imread(input_data)\n",
    "        if img_bgr is None:\n",
    "            print(f\"[Warning] Cannot read image: {input_data}\")\n",
    "            return None\n",
    "        img_path = input_data\n",
    "    elif isinstance(input_data, np.ndarray):\n",
    "        img_bgr = input_data.copy()\n",
    "        img_path = \"<array_input>\"\n",
    "    else:\n",
    "        raise ValueError(\"input_data must be either a file path (str) or an image (np.ndarray)\")\n",
    "\n",
    "    # --- Normalize / preprocess ---\n",
    "    img_bgr = normalize_hand_image(\n",
    "        img_bgr,\n",
    "        use_clahe=use_clahe,\n",
    "        clahe_clip=clahe_clip,\n",
    "        clahe_tile=clahe_tile\n",
    "    )\n",
    "\n",
    "    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "    h, w = img_bgr.shape[:2]\n",
    "\n",
    "    # --- Run MediaPipe Hands ---\n",
    "    res = hands.process(img_rgb)\n",
    "    if not res.multi_hand_landmarks:\n",
    "        return {\"img\": img_bgr, \"n_hands\": 0, \"img_path\": img_path}\n",
    "\n",
    "    outputs = []\n",
    "    base_bgr = img_bgr\n",
    "\n",
    "    # --- Process each detected hand ---\n",
    "    for i, lms in enumerate(res.multi_hand_landmarks):\n",
    "        handed_label = res.multi_handedness[i].classification[0].label\n",
    "        handed_score = res.multi_handedness[i].classification[0].score\n",
    "\n",
    "        xs = np.array([lm.x for lm in lms.landmark], dtype=np.float32) * w\n",
    "        ys = np.array([lm.y for lm in lms.landmark], dtype=np.float32) * h\n",
    "        zs = np.array([lm.z for lm in lms.landmark], dtype=np.float32)\n",
    "        LM = np.stack([xs, ys, zs], axis=1)\n",
    "\n",
    "        # Reject small hand boxes\n",
    "        bw, bh = xs.max()-xs.min(), ys.max()-ys.min()\n",
    "        if (bw * bh) < (min_bbox_frac * w * h):\n",
    "            continue\n",
    "\n",
    "        # Canonicalize left→right\n",
    "        LM_canon = LM.copy()\n",
    "        if handed_label == \"Left\":\n",
    "            LM_canon[:, 0] = w - LM_canon[:, 0]\n",
    "            LM_canon[:, 2] *= -1\n",
    "\n",
    "        feat63 = landmarks_to_feature(LM_canon)\n",
    "\n",
    "        mask = grabcut_mask_from_landmarks(base_bgr, LM)\n",
    "        overlay = draw_landmarks_bgr(base_bgr.copy(), LM)\n",
    "\n",
    "        outputs.append({\n",
    "            \"handed_label\": handed_label,\n",
    "            \"handed_score\": float(handed_score),\n",
    "            \"landmarks_px_xyz\": LM,\n",
    "            \"landmarks_px_xyz_canon\": LM_canon,\n",
    "            \"feature63\": feat63,\n",
    "            \"mask\": mask,\n",
    "            \"overlay\": overlay\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        \"img\": base_bgr,\n",
    "        \"n_hands\": len(outputs),\n",
    "        \"hands\": outputs,\n",
    "        \"img_path\": img_path\n",
    "    }\n",
    "\n",
    "\n",
    "# -------- Batch runner --------\n",
    "def run_folder(input_glob, out_dir=\"out_mediapipe\", max_hands=1, det_conf=0.5):\n",
    "    ensure_dir(out_dir)\n",
    "    vis_dir   = ensure_dir(os.path.join(out_dir, \"overlay\"))\n",
    "    mask_dir  = ensure_dir(os.path.join(out_dir, \"mask\"))\n",
    "    csv_path  = os.path.join(out_dir, \"features63.csv\")\n",
    "\n",
    "    mp_hands = mp.solutions.hands\n",
    "    all_rows = []\n",
    "    with mp_hands.Hands(static_image_mode=True,\n",
    "                        max_num_hands=max_hands,\n",
    "                        min_detection_confidence=det_conf,model_complexity=1) as hands:\n",
    "        img_paths = sorted(glob.glob(input_glob))\n",
    "        for p in img_paths:\n",
    "            r = process_image_with_mediapipe(p, hands)\n",
    "            if r is None: \n",
    "                print(f\"[skip] cannot read {p}\")\n",
    "                continue\n",
    "            if r[\"n_hands\"] == 0:\n",
    "                print(f\"[no hand] {p}\")\n",
    "                continue\n",
    "            for idx, hinfo in enumerate(r[\"hands\"]):\n",
    "                base = os.path.splitext(os.path.basename(p))[0]\n",
    "                savedDir = os.path.join(mask_dir, f\"{base}_hand{idx}.png\")\n",
    "                saveLayout = os.path.join(vis_dir, f\"{base}_hand{idx}.jpg\")\n",
    "                # save overlay & mask\n",
    "                cv2.imwrite(saveLayout, hinfo[\"overlay\"])\n",
    "                cv2.imwrite(savedDir, hinfo[\"mask\"])\n",
    "                # save feature row\n",
    "                row = [saveLayout, idx] + hinfo[\"feature63\"].astype(float).tolist()\n",
    "                all_rows.append(row)\n",
    "\n",
    "    # write CSV\n",
    "    header = [\"path\", \"hand_idx\"] + [f\"f{i:02d}\" for i in range(63)]\n",
    "    with open(csv_path, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f); writer.writerow(header); writer.writerows(all_rows)\n",
    "    print(f\"[done] saved overlays→ {vis_dir}, masks→ {mask_dir}, features→ {csv_path}\")\n",
    "\n",
    "\n",
    "# # # uncomment to run the meidapipe processing on a folder of images\n",
    "# # # ---------- Run ----------\n",
    "# base_dir = r\"D:/5710/MajorProj/Type_01\"\n",
    "# base_savedir = r\"D:/5710/MajorProj/MediaPipeOutput\"\n",
    "\n",
    "# # Loop through all subfolders\n",
    "# for folder_name in os.listdir(base_dir):\n",
    "#     subfolder_path = os.path.join(base_dir, folder_name)\n",
    "#     if not os.path.isdir(subfolder_path):\n",
    "#         continue  # skip files\n",
    "\n",
    "#     # Input image pattern (all JPGs)\n",
    "#     in_pattern = os.path.join(subfolder_path, \"*.jpg\")\n",
    "\n",
    "#     # Output directory name pattern: (folder_name)out_mediapipe_(folder_name)\n",
    "#     out_dir = os.path.join(base_savedir, f\"{folder_name}out_mediapipe_{folder_name}\")\n",
    "\n",
    "#     # Make sure the output folder exists\n",
    "#     os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "#     print(f\"Processing folder: {folder_name}\")\n",
    "#     print(f\"  Input: {in_pattern}\")\n",
    "#     print(f\"  Output: {out_dir}\")\n",
    "\n",
    "#     # Run your function\n",
    "#     run_folder(in_pattern, out_dir=out_dir, max_hands=1, det_conf=0.8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef25da8",
   "metadata": {},
   "source": [
    "### Hand Guesture Normalizer\n",
    "The Posture reached is required to be normalized and rotated to the standard posture. The following Helper function performs this operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d3bf99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Indices (MediaPipe Hands) ----\n",
    "WRIST = 0\n",
    "MCP_INDEX, MCP_MIDDLE, MCP_RING, MCP_PINKY = 5, 9, 13, 17\n",
    "TIP_THUMB = 4\n",
    "TIP_IDS = [4, 8, 12, 16, 20]  # thumb→pinky tips\n",
    "\n",
    "# ---- Small helpers ----\n",
    "def _nz(v, eps=1e-9):\n",
    "    n = np.linalg.norm(v)\n",
    "    return v / (n + eps)\n",
    "\n",
    "def mp_to_camera_frame(L, img_wh=None, assume_normalized=False):\n",
    "    \"\"\"x right, y UP, z positive towards camera; centered & roughly unit scale.\"\"\"\n",
    "    P = L.astype(np.float32).copy()\n",
    "    if img_wh is not None and not assume_normalized:\n",
    "        w, h = float(img_wh[0]), float(img_wh[1])\n",
    "        s = max(w, h)\n",
    "        P[:, 0] = (P[:, 0] - w/2.0) / s\n",
    "        P[:, 1] = (h/2.0 - P[:, 1]) / s    # flip Y and center\n",
    "    else:\n",
    "        P[:, 0] = P[:, 0] - 0.5\n",
    "        P[:, 1] = 0.5 - P[:, 1]\n",
    "    P[:, 2] = -P[:, 2]                    # closer = positive\n",
    "    return P\n",
    "\n",
    "\n",
    "def kabsch(A, B, with_scale=False):\n",
    "    \"\"\"Align A->B via rigid (or similarity) transform; returns (R, t, s).\"\"\"\n",
    "    A = A.astype(np.float32); B = B.astype(np.float32)\n",
    "    cA, cB = A.mean(0), B.mean(0)\n",
    "    A0, B0 = A - cA, B - cB\n",
    "    if with_scale:\n",
    "        sA = np.sqrt((A0**2).sum()/A.shape[0]) + 1e-9\n",
    "        sB = np.sqrt((B0**2).sum()/A.shape[0]) + 1e-9\n",
    "        A0 /= sA; B0 /= sB\n",
    "    H = A0.T @ B0\n",
    "    U, S, Vt = np.linalg.svd(H)\n",
    "    R = Vt.T @ U.T\n",
    "    if np.linalg.det(R) < 0:\n",
    "        Vt[-1, :] *= -1\n",
    "        R = Vt.T @ U.T\n",
    "    s = 1.0\n",
    "    if with_scale:\n",
    "        s = (S.sum() / (A0**2).sum())\n",
    "    t = cB - (s * (R @ cA))\n",
    "    return R.astype(np.float32), t.astype(np.float32), float(s)\n",
    "\n",
    "# ---- Your upgraded normalizer ----\n",
    "def normalize_hand_orientation(\n",
    "    landmarks,\n",
    "    img_wh=None,\n",
    "    assume_normalized_xy=False,\n",
    "    method=\"basis\",            # 'basis' (fast) or 'kabsch' (template)\n",
    "    template=None,             # (21,3) template in camera frame if method='kabsch'\n",
    "    kabsch_with_scale=False,   # use similarity instead of pure rigid\n",
    "    mirror_thumb=True          # enforce thumb on +X\n",
    "):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "      landmarks: (21,3) from MediaPipe (x,y in px or [0..1], z relative).\n",
    "      img_wh: (W,H) if landmarks are in pixels (recommended).\n",
    "      assume_normalized_xy: set True if x,y already in [0..1].\n",
    "      method: 'basis' (default) or 'kabsch'.\n",
    "      template: required if method='kabsch'; reference (21,3) in camera frame.\n",
    "    Returns:\n",
    "      Lc: (21,3) canonicalized (wrist@origin, scaled, 'pointing up', thumb on +X).\n",
    "      info: dict with 'R' (3x3), 't' (3,), 'scale' (float), and 'method'.\n",
    "    \"\"\"\n",
    "\n",
    "    assert landmarks.shape == (21, 3)\n",
    "    # 0) Convert to right-handed camera frame\n",
    "    P = mp_to_camera_frame(landmarks, img_wh=img_wh, assume_normalized=assume_normalized_xy)\n",
    "    # 1) Translate to wrist origin\n",
    "    P -= P[WRIST]\n",
    "    # 2) Scale by wrist->middle-MCP (robust hand size)\n",
    "    hand_size = np.linalg.norm(P[MCP_MIDDLE]) or 1.0\n",
    "    P /= hand_size\n",
    "\n",
    "    R = np.eye(3, dtype=np.float32)\n",
    "    t = np.zeros(3, dtype=np.float32)\n",
    "    s = 1.0\n",
    "\n",
    "    if method == \"basis\":\n",
    "        # --- Orthonormal hand basis ---\n",
    "        v = _nz(P[MCP_MIDDLE])  # desired +Y\n",
    "        u = _nz((P[MCP_INDEX] - P[WRIST]) - (P[MCP_PINKY] - P[WRIST]))  # desired +X\n",
    "        w = _nz(np.cross(u, v))  # desired +Z\n",
    "        u = _nz(np.cross(v, w))  # re-orthogonalize\n",
    "        B = np.stack([u, v, w], axis=1)  # columns are axes\n",
    "        Lc = P @ B                       # rotate into canonical (u,v,w)\n",
    "        R = B\n",
    "    else:\n",
    "        # --- Kabsch alignment to template ---\n",
    "        assert template is not None and template.shape == (21,3), \\\n",
    "            \"Provide a (21,3) template in camera frame for method='kabsch'.\"\n",
    "\n",
    "        # Ensure template is wrist-centered and comparable scale\n",
    "        T = template.astype(np.float32).copy()\n",
    "        T -= T[WRIST]\n",
    "        Ts = np.linalg.norm(T[MCP_MIDDLE]) or 1.0\n",
    "        T /= Ts\n",
    "\n",
    "        IDX = [WRIST, MCP_INDEX, MCP_MIDDLE, MCP_RING, MCP_PINKY]\n",
    "        Rk, tk, sk = kabsch(P[IDX], T[IDX], with_scale=kabsch_with_scale)\n",
    "        Lc = (sk * (P @ Rk.T)) + tk\n",
    "        R, t, s = Rk, tk, sk\n",
    "\n",
    "    # 3) Mirror so thumb is on +X (consistent handedness)\n",
    "    if mirror_thumb and Lc[TIP_THUMB, 0] < 0:\n",
    "        Lc[:, 0] *= -1.0\n",
    "        R[:, 0] *= -1.0  # reflect basis X to keep R consistent\n",
    "\n",
    "    info = {\"R\": R, \"t\": t, \"scale\": s * (1.0/hand_size), \"method\": method}\n",
    "    return Lc, info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ff85b8",
   "metadata": {},
   "source": [
    "### Perform the preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8f2381d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 folders to process.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_proximal_phalange_lengths(L):\n",
    "    \"\"\"\n",
    "    Compute normalized proximal phalange lengths for each finger.\n",
    "    L : (21,3) landmark array (after normalization).\n",
    "    Returns dict with one length per finger.\n",
    "    \"\"\"\n",
    "    # Indices per finger (MediaPipe)\n",
    "    WRIST = 0\n",
    "    MCP = [5, 9, 13, 17]   # base of each finger (Index–Pinky)\n",
    "    PIP = [6, 10, 14, 18]  # next joint up\n",
    "\n",
    "    lengths = {}\n",
    "    for name, mcp, pip in zip([\"Index\", \"Middle\", \"Ring\", \"Pinky\"], MCP, PIP):\n",
    "        lengths[name] = float(np.linalg.norm(L[pip] - L[mcp]))\n",
    "    # Thumb: use 1→2 (MCP→IP)\n",
    "    lengths[\"Thumb\"] = float(np.linalg.norm(L[2] - L[1]))\n",
    "    return lengths\n",
    "\n",
    "def get_middle_phalange_lengths(L):\n",
    "    \"\"\"\n",
    "    Compute normalized middle phalange lengths for each finger.\n",
    "    L : (21,3) landmark array (after normalization).\n",
    "    Returns dict with one length per finger.\n",
    "    \"\"\"\n",
    "    # indices for middle phalanges (PIP→DIP)\n",
    "    FINGERS = {\n",
    "        \"Thumb\": (2, 4),    # IP→Tip\n",
    "        \"Index\": (6, 7),    # PIP→DIP\n",
    "        \"Middle\": (10, 11),\n",
    "        \"Ring\": (14, 15),\n",
    "        \"Pinky\": (18, 19)\n",
    "    }\n",
    "\n",
    "    lengths = {}\n",
    "    for name, (a, b) in FINGERS.items():\n",
    "        lengths[name] = float(np.linalg.norm(L[b] - L[a]))\n",
    "    return lengths\n",
    "\n",
    "\n",
    "def joint_distance(L, i, j):\n",
    "    \"\"\"\n",
    "    Compute the Euclidean distance between two joints in a (21,3) landmark array.\n",
    "    L : np.ndarray of shape (21,3)\n",
    "    i, j : joint indices (e.g., 0=wrist, 9=middle MCP, etc.)\n",
    "    Returns: distance (float)\n",
    "    \"\"\"\n",
    "    assert L.shape == (21, 3), \"Input must be (21,3) landmarks.\"\n",
    "    p1, p2 = L[i], L[j]\n",
    "    dist = np.linalg.norm(p1 - p2)\n",
    "    print(f\"Distance between joint {i} and {j}: {dist:.5f}\")\n",
    "    return dist\n",
    "\n",
    "\n",
    "def mp_to_camera_frame(L, img_wh=None, assume_normalized=False):\n",
    "    \"\"\"\n",
    "    Convert MediaPipe landmarks to a right-handed camera frame for plotting.\n",
    "    MediaPipe: x right, y DOWN, z negative = closer.\n",
    "    Output:    x right, y UP,   z positive = closer.\n",
    "\n",
    "    L : (21,3) array in pixels (if img_wh given) or normalized [0,1] (if assume_normalized=True).\n",
    "    img_wh : (w,h) if L is in pixels.\n",
    "    \"\"\"\n",
    "    P = L.astype(np.float32).copy()\n",
    "\n",
    "    if img_wh is not None and not assume_normalized:\n",
    "        w, h = float(img_wh[0]), float(img_wh[1])\n",
    "        s = max(w, h)  # isotropic scale, keeps aspect\n",
    "        # centre to (0,0) and scale to ~[-0.5,0.5]\n",
    "        P[:, 0] = (P[:, 0] - w/2.0) / s\n",
    "        P[:, 1] = (h/2.0 - P[:, 1]) / s   # flip Y (down->up) and centre\n",
    "    else:\n",
    "        # normalized [0,1] -> centre to 0 and flip Y\n",
    "        P[:, 0] = P[:, 0] - 0.5\n",
    "        P[:, 1] = 0.5 - P[:, 1]           # flip Y\n",
    "\n",
    "    P[:, 2] = -P[:, 2]                    # make positive Z = closer\n",
    "    return P\n",
    "\n",
    "def plot_hand_3d_fixed(L, img_wh=None, assume_normalized=False,\n",
    "                       title=\"Hand (right-handed camera frame)\",\n",
    "                       elev=20, azim=120, save_path=None):\n",
    "    \"\"\"\n",
    "    L : (21,3) MediaPipe landmarks (pixels or normalized).\n",
    "    img_wh : (w,h) if L is in pixels; else None.\n",
    "    assume_normalized : True if L[:,0:2] are in [0,1].\n",
    "    \"\"\"\n",
    "    assert L.shape == (21,3), \"Expected (21,3) landmarks.\"\n",
    "\n",
    "    P = mp_to_camera_frame(L, img_wh=img_wh, assume_normalized=assume_normalized)\n",
    "    xs, ys, zs = P[:,0], P[:,1], P[:,2]\n",
    "\n",
    "    fig = plt.figure(figsize=(6,6))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.view_init(elev=elev, azim=azim)\n",
    "    ax.set_title(title)\n",
    "\n",
    "    # --- show phalange lengths ---\n",
    "    lengths = get_proximal_phalange_lengths(L)\n",
    "    print(\"\\n--- Normalized proximal phalange lengths ---\")\n",
    "    for finger, val in lengths.items():\n",
    "        print(f\"{finger:<7s}: {val:.4f}\")\n",
    "\n",
    "    print(\"\\n--- Normalized middle phalange lengths ---\")\n",
    "    mid_lengths = get_middle_phalange_lengths(L)\n",
    "    for finger, val in mid_lengths.items():\n",
    "        print(f\"{finger:<7s}: {val:.4f}\")\n",
    "\n",
    "    # --- plot bones ---\n",
    "    for i, j in HAND_CONNECTIONS:\n",
    "        ax.plot([xs[i], xs[j]], [ys[i], ys[j]], [zs[i], zs[j]], lw=2)\n",
    "\n",
    "    # --- plot joints and labels ---\n",
    "    ax.scatter(xs, ys, zs, c='r', s=40)\n",
    "    for idx in range(len(xs)):\n",
    "        ax.text(xs[idx], ys[idx], zs[idx],\n",
    "                f\"{idx}\", color='black', fontsize=8, ha='center')\n",
    "\n",
    "    # mark wrist clearly\n",
    "    ax.text(xs[0], ys[0], zs[0], \"Wrist\", color='blue', fontsize=9, fontweight='bold')\n",
    "\n",
    "    # --- set axis and limits ---\n",
    "    ax.set_xlim(-0.8, 0.8)\n",
    "    ax.set_ylim(-0.8, 0.8)\n",
    "    ax.set_zlim(-0.8, 0.8)\n",
    "    ax.set_box_aspect([1,1,1])\n",
    "    ax.set_xlabel(\"X (right)\")\n",
    "    ax.set_ylabel(\"Y (up)\")\n",
    "    ax.set_zlabel(\"Z (towards camera)\")\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=160, bbox_inches=\"tight\")\n",
    "        plt.close(fig)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- 1  Source & destination paths ---\n",
    "src_root = r\"D:\\Files\\2025_Y4_S2\\AMME5710\\Major\\MediaPipeOutput\"\n",
    "dst_root = r\"D:\\Files\\2025_Y4_S2\\AMME5710\\Major\\NormalizedOutput\"\n",
    "\n",
    "# --- 2  Find all subfolder CSVs ---\n",
    "csv_files = glob.glob(os.path.join(src_root, \"*\", \"features63.csv\"))\n",
    "print(f\"Found {len(csv_files)} folders to process.\")\n",
    "\n",
    "# --- 3  Utility functions ---\n",
    "\n",
    "\n",
    "def ensure_dir(path):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    return path\n",
    "\n",
    "def mirror_and_save(Lc, src_img_path, out_root, class_name):\n",
    "    \"\"\"\n",
    "    Save normalized image + 63D CSV row to mirrored subfolder.\n",
    "    class_name may look like 'Aout_mediapipe_A' — we only keep 'A'.\n",
    "    \"\"\"\n",
    "    # --- clean class_name ---\n",
    "    # keep the part AFTER the last occurrence of '_mediapipe_'\n",
    "    if \"_mediapipe_\" in class_name:\n",
    "        class_name = class_name.split(\"_mediapipe_\")[-1]\n",
    "\n",
    "    # proceed as before\n",
    "    base = os.path.splitext(os.path.basename(src_img_path))[0]\n",
    "    out_dir = ensure_dir(os.path.join(out_root, class_name))\n",
    "    out_img_dir = ensure_dir(os.path.join(out_dir, \"images\"))\n",
    "    out_csv = os.path.join(out_dir, \"features63_normalized.csv\")\n",
    "\n",
    "    # --- copy image ---\n",
    "    dst_img = os.path.join(out_img_dir, f\"{base}_norm.jpg\")\n",
    "    img = cv2.imread(src_img_path)\n",
    "    if img is not None:\n",
    "        cv2.imwrite(dst_img, img)\n",
    "\n",
    "    # --- flatten 63 features and append to CSV ---\n",
    "    feat = Lc.reshape(-1).astype(float).tolist()\n",
    "    header = [\"path\"] + [f\"f{i:02d}\" for i in range(63)]\n",
    "    row = [dst_img] + feat\n",
    "\n",
    "    new_file = not os.path.exists(out_csv)\n",
    "    with open(out_csv, \"a\", newline=\"\") as f:\n",
    "        import csv\n",
    "        writer = csv.writer(f)\n",
    "        if new_file:\n",
    "            writer.writerow(header)\n",
    "        writer.writerow(row)\n",
    "\n",
    "    return dst_img, out_csv\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # --- 4  Process each CSV file ---\n",
    "# overall_total = 0\n",
    "# overall_rejected = 0\n",
    "# overall_accepted = 0\n",
    "\n",
    "# for csv_path in tqdm(csv_files, desc=\"Processing folders\"):\n",
    "#     gate = AutoGate.RANSACRejectionGate(\n",
    "#         win=512, ransac_iters=60, ransac_thresh=0.06,\n",
    "#         k_score=3.0, close_ratio=0.04, min_inliers=4,\n",
    "#         template_palm=None\n",
    "#     )\n",
    "\n",
    "#     class_name = os.path.basename(os.path.dirname(csv_path))\n",
    "#     df = pd.read_csv(csv_path)\n",
    "\n",
    "#     X = df.loc[:, \"f00\":\"f62\"].to_numpy(dtype=np.float32)\n",
    "#     FileNames = df[\"path\"].to_numpy()\n",
    "#     print(f\"\\n[{class_name}] → {len(X)} samples\")\n",
    "\n",
    "#     total = len(X)\n",
    "#     accepted = 0\n",
    "#     rejected = 0\n",
    "\n",
    "#     for i, sample in enumerate(X):\n",
    "#         L = sample.reshape((21, 3))\n",
    "#         Lc, info = normalize_hand_orientation(L, method=\"basis\",\n",
    "#                                               kabsch_with_scale=True,\n",
    "#                                               mirror_thumb=True)\n",
    "#         z_zoomfactor = 2.5\n",
    "#         Lc[:, 0] *= z_zoomfactor\n",
    "\n",
    "#         is_ok = gate.is_valid(Lc)\n",
    "#         if not is_ok:\n",
    "#             rejected += 1\n",
    "#             continue\n",
    "#         accepted += 1\n",
    "#         mirror_and_save(Lc, FileNames[i], dst_root, class_name)\n",
    "\n",
    "#     # class summary\n",
    "#     acc_rate = accepted / total if total else 0\n",
    "#     rej_rate = rejected / total if total else 0\n",
    "#     print(f\"[{class_name}] accepted={accepted}  rejected={rejected}  \"\n",
    "#           f\"accept_rate={acc_rate:.3f}  reject_rate={rej_rate:.3f}\")\n",
    "\n",
    "#     # accumulate global stats\n",
    "#     overall_total += total\n",
    "#     overall_accepted += accepted\n",
    "#     overall_rejected += rejected\n",
    "\n",
    "# # ---- final overall summary ----\n",
    "# if overall_total > 0:\n",
    "#     overall_acc_rate = overall_accepted / overall_total\n",
    "#     overall_rej_rate = overall_rejected / overall_total\n",
    "#     print(\"\\n===== Overall Summary =====\")\n",
    "#     print(f\"Total samples     : {overall_total}\")\n",
    "#     print(f\"Accepted samples  : {overall_accepted}\")\n",
    "#     print(f\"Rejected samples  : {overall_rejected}\")\n",
    "#     print(f\"Acceptance rate   : {overall_acc_rate:.3%}\")\n",
    "#     print(f\"Rejection rate    : {overall_rej_rate:.3%}\")\n",
    "# else:\n",
    "#     print(\"No samples processed.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8082bd1b",
   "metadata": {},
   "source": [
    "### Data preparation and feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa7085ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "# =========================\n",
    "# Config\n",
    "# =========================\n",
    "BASE_FOLDER   = r\"D:\\Files\\2025_Y4_S2\\AMME5710\\Major\"\n",
    "NORM_ROOT     = os.path.join(BASE_FOLDER, \"NormalizedOutput\")  # where features63_normalized.csv live (per class)\n",
    "TYPE01_ROOT   = os.path.join(BASE_FOLDER, \"Type_01\")          # original images root by class (A,B,C,...)\n",
    "OUT_CSV       = os.path.join(NORM_ROOT, \"combined_features.csv\")\n",
    "POSSIBLE_EXTS = [\".jpg\", \".png\", \".jpeg\", \".JPG\", \".PNG\", \".JPEG\"]\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Hand topology / helpers\n",
    "# =========================\n",
    "# MediaPipe Hands indices\n",
    "WRIST = 0\n",
    "MCP_INDEX, MCP_MIDDLE, MCP_RING, MCP_PINKY = 5, 9, 13, 17\n",
    "TIP_THUMB, TIP_INDEX, TIP_MIDDLE, TIP_RING, TIP_PINKY = 4, 8, 12, 16, 20\n",
    "\n",
    "# Edges for bone-length features (21 joints)\n",
    "EDGES = [\n",
    "    (0,1),(1,2),(2,3),(3,4),          # thumb\n",
    "    (0,5),(5,6),(6,7),(7,8),          # index\n",
    "    (0,9),(9,10),(10,11),(11,12),     # middle\n",
    "    (0,13),(13,14),(14,15),(15,16),   # ring\n",
    "    (0,17),(17,18),(18,19),(19,20)    # pinky\n",
    "]\n",
    "\n",
    "FINGER_CHAINS = {\n",
    "    \"thumb\":  [0,1,2,3,4],\n",
    "    \"index\":  [0,5,6,7,8],\n",
    "    \"middle\": [0,9,10,11,12],\n",
    "    \"ring\":   [0,13,14,15,16],\n",
    "    \"pinky\":  [0,17,18,19,20],\n",
    "}\n",
    "\n",
    "def _angle(u, v, eps=1e-9):\n",
    "    \"\"\"Angle between u and v in radians.\"\"\"\n",
    "    nu = np.linalg.norm(u) + eps\n",
    "    nv = np.linalg.norm(v) + eps\n",
    "    cos = np.clip(np.dot(u, v) / (nu * nv), -1.0, 1.0)\n",
    "    return float(np.arccos(cos))\n",
    "\n",
    "def _palm_quad_area_2d(L21x3):\n",
    "    \"\"\"Area from MCP quad (index, middle, ring, pinky) using XY only.\"\"\"\n",
    "    A = L21x3[[MCP_INDEX, MCP_MIDDLE, MCP_RING, MCP_PINKY], :2]\n",
    "    # shoelace\n",
    "    x, y = A[:,0], A[:,1]\n",
    "    return 0.5 * abs(np.dot(x, np.roll(y, -1)) - np.dot(y, np.roll(x, -1)))\n",
    "\n",
    "def _joint_angles(L21x3):\n",
    "    \"\"\"\n",
    "    Per-finger joint angles at MCP/PIP/DIP (thumb has CMC/MCP/IP).\n",
    "    Returns dict of angles (radians).\n",
    "    \"\"\"\n",
    "    ang = {}\n",
    "    for name, chain in FINGER_CHAINS.items():\n",
    "        # segments: wrist->1, 1->2, 2->3, 3->tip\n",
    "        pts = L21x3[chain]\n",
    "        v01 = pts[1] - pts[0]\n",
    "        v12 = pts[2] - pts[1]\n",
    "        v23 = pts[3] - pts[2]\n",
    "        v34 = pts[4] - pts[3]\n",
    "        # internal angles at joints 1,2,3 using incoming/outgoing vectors\n",
    "        a1 = _angle(-v01, v12)  # joint at node 1\n",
    "        a2 = _angle(-v12, v23)  # joint at node 2\n",
    "        a3 = _angle(-v23, v34)  # joint at node 3\n",
    "        ang[f\"{name}_j1\"] = a1\n",
    "        ang[f\"{name}_j2\"] = a2\n",
    "        ang[f\"{name}_j3\"] = a3\n",
    "    return ang\n",
    "\n",
    "def _spread_measures(L21x3):\n",
    "    \"\"\"\n",
    "    Finger-spread metrics using MCP points in XY (scale-invariant if your L21x3 is normalized).\n",
    "    \"\"\"\n",
    "    P = L21x3\n",
    "    mcp = P[[MCP_INDEX, MCP_MIDDLE, MCP_RING, MCP_PINKY], :2]\n",
    "    d_idx_mid   = np.linalg.norm(mcp[1] - mcp[0])\n",
    "    d_mid_ring  = np.linalg.norm(mcp[2] - mcp[1])\n",
    "    d_ring_pink = np.linalg.norm(mcp[3] - mcp[2])\n",
    "    return {\n",
    "        \"spread_idx_mid\":   float(d_idx_mid),\n",
    "        \"spread_mid_ring\":  float(d_mid_ring),\n",
    "        \"spread_ring_pink\": float(d_ring_pink),\n",
    "    }\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Image-only features (global; ROI-free)\n",
    "# =========================\n",
    "\n",
    "def _img_features(img_bgr):\n",
    "    \"\"\"\n",
    "    Image-level stats that do not need the hand ROI:\n",
    "      - brightness mean/std (gray)\n",
    "      - Laplacian variance (sharpness)\n",
    "      - edges density (Canny on gray)\n",
    "      - HSV means/stds\n",
    "    \"\"\"\n",
    "    feats = {}\n",
    "    if img_bgr is None or img_bgr.size == 0:\n",
    "        # NaNs to make it obvious we couldn't read the image\n",
    "        for k in [\n",
    "            \"lap_var\",\"edge_density\"\n",
    "        ]:\n",
    "            feats[f\"img_{k}\"] = np.nan\n",
    "        return feats\n",
    "\n",
    "    h, w = img_bgr.shape[:2]\n",
    "    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    # Sharpness\n",
    "    feats[\"img_lap_var\"] = float(cv2.Laplacian(gray, cv2.CV_64F).var())\n",
    "\n",
    "    # Edge density (Canny thresholds tied to median)\n",
    "    med = np.median(gray)\n",
    "    low = int(max(0, 0.66 * med))\n",
    "    high = int(min(255, 1.33 * med))\n",
    "    edges = cv2.Canny(gray, low, high)\n",
    "    feats[\"img_edge_density\"] = float((edges > 0).mean())\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "# =========================\n",
    "# FeatureCombiner\n",
    "# =========================\n",
    "\n",
    "class FeatureCombiner:\n",
    "    \"\"\"\n",
    "    Receives:\n",
    "      - img_bgr: original image (OpenCV BGR)\n",
    "      - feat63:  (63,) flattened [x,y,z]*21  (typically canonical/normalized in your pipeline)\n",
    "    Returns:\n",
    "      dict of features (geometry + image stats).\n",
    "    \"\"\"\n",
    "    def __init__(self, include_original63=False):\n",
    "        self.include_original63 = include_original63\n",
    "\n",
    "    def compute(self, img_bgr, feat63):\n",
    "        out = {}\n",
    "\n",
    "        # 1) unpack landmarks\n",
    "        L = np.asarray(feat63, dtype=np.float32).reshape(21, 3)  # normalized/canonical in your pipeline\n",
    "\n",
    "        # 2) geometry features (scale-invariant on normalized L)\n",
    "\n",
    "        #    b) joint angles per finger (MCP/PIP/DIP)\n",
    "        ang = _joint_angles(L)\n",
    "        for k, v in ang.items():\n",
    "            out[f\"geo_angle_{k}\"] = float(v)\n",
    "\n",
    "        #    c) palm area + spread\n",
    "        out[\"geo_palm_area\"] = float(_palm_quad_area_2d(L))\n",
    "        spread = _spread_measures(L)\n",
    "        for k, v in spread.items():\n",
    "            out[f\"geo_{k}\"] = float(v)\n",
    "\n",
    "        #    d) z-shape stats\n",
    "        z = L[:, 2]\n",
    "        out[\"geo_z_mean\"] = float(np.mean(z))\n",
    "        out[\"geo_z_std\"]  = float(np.std(z))\n",
    "        out[\"geo_z_min\"]  = float(np.min(z))\n",
    "        out[\"geo_z_max\"]  = float(np.max(z))\n",
    "        out[\"geo_z_rng\"]  = out[\"geo_z_max\"] - out[\"geo_z_min\"]\n",
    "\n",
    "        # 3) image features (global)\n",
    "        out.update(_img_features(img_bgr))\n",
    "\n",
    "        # 4) (optional) keep the raw 63D for traceability (prefix 'raw_')\n",
    "        if self.include_original63:\n",
    "            for i, val in enumerate(np.asarray(feat63, dtype=np.float32).ravel()):\n",
    "                out[f\"raw_f{i:02d}\"] = float(val)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3631a325",
   "metadata": {},
   "source": [
    "### CSV reader and saver with Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d138ce82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# CSV utilities  +  Build the training-test dataset\n",
    "# =========================\n",
    "def load_per_class_csvs(norm_root):\n",
    "    \"\"\"\n",
    "    Finds all subfolders under norm_root and loads 'features63_normalized.csv' if present.\n",
    "    Returns list of (class_name, DataFrame, csv_path, class_dir).\n",
    "    \"\"\"\n",
    "    bundles = []\n",
    "    for cdir in sorted(glob.glob(os.path.join(norm_root, \"*\"))):\n",
    "        if not os.path.isdir(cdir):\n",
    "            continue\n",
    "        cname = os.path.basename(cdir)\n",
    "        csv_path = os.path.join(cdir, \"features63_normalized.csv\")\n",
    "        if os.path.exists(csv_path):\n",
    "            df = pd.read_csv(csv_path)\n",
    "            bundles.append((cname, df, csv_path, cdir))\n",
    "    return bundles\n",
    "\n",
    "def find_original_image_path(stored_path, class_name):\n",
    "    \"\"\"\n",
    "    Try to reconstruct the original image path in Type_01/class_name/<token>.<ext>\n",
    "    from a saved overlay/normalized path that contains '<class>_<id>_hand0.ext'.\n",
    "    \"\"\"\n",
    "    fname = os.path.basename(stored_path)\n",
    "    m = re.match(rf\"({class_name}_[0-9]+)_hand\\d+\\.[^.]+$\", fname, flags=re.IGNORECASE)\n",
    "    if m:\n",
    "        token = m.group(1)\n",
    "    else:\n",
    "        # fallback: strip trailing '_hand...' if present\n",
    "        stem, _ = os.path.splitext(fname)\n",
    "        token = stem.split(\"_hand\")[0]\n",
    "\n",
    "    # Try all common extensions\n",
    "    for ext in POSSIBLE_EXTS:\n",
    "        candidate = os.path.join(TYPE01_ROOT, class_name, token + ext)\n",
    "        if os.path.exists(candidate):\n",
    "            return candidate\n",
    "    # if nothing exists, return best guess (jpg) so we at least write something\n",
    "    return os.path.join(TYPE01_ROOT, class_name, token + \".jpg\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Main: combine features -> CSV\n",
    "# =========================\n",
    "def main():\n",
    "    combiner = FeatureCombiner(include_original63=False)\n",
    "    bundles = load_per_class_csvs(NORM_ROOT)\n",
    "    if not bundles:\n",
    "        print(f\"[error] No per-class normalized CSVs found under: {NORM_ROOT}\")\n",
    "        return\n",
    "\n",
    "    rows_out = []\n",
    "    for class_name, df, _, _ in bundles:\n",
    "        # Expect \"path\" + f00..f62\n",
    "        if \"path\" not in df.columns:\n",
    "            print(f\"[warn] 'path' missing in {class_name} CSV — skipping class\")\n",
    "            continue\n",
    "\n",
    "        # iterate rows\n",
    "        for idx, row in df.iterrows():\n",
    "            stored_path = row[\"path\"]\n",
    "            # recover original image\n",
    "            orig_path = find_original_image_path(stored_path, class_name)\n",
    "            img = cv2.imread(orig_path)\n",
    "\n",
    "            # extract the 63-D feature vector (from f00..f62 or raw_f00..f62)\n",
    "            if \"f00\" in row.index:\n",
    "                feat = np.array([row[f\"f{j:02d}\"] for j in range(63)], dtype=np.float32)\n",
    "            elif \"raw_f00\" in row.index:\n",
    "                feat = np.array([row[f\"raw_f{j:02d}\"] for j in range(63)], dtype=np.float32)\n",
    "            else:\n",
    "                # try to auto-detect\n",
    "                fcols = [c for c in row.index if re.fullmatch(r\"(raw_)?f\\d{2}\", c)]\n",
    "                fcols = sorted(fcols, key=lambda x: int(re.findall(r\"\\d{2}\", x)[0]))\n",
    "                feat = row[fcols].to_numpy(np.float32)\n",
    "                if feat.size != 63:\n",
    "                    print(f\"[warn] row {idx} in {class_name}: cannot find 63-D vector; skip\")\n",
    "                    continue\n",
    "\n",
    "            # compute combined features\n",
    "            feats = combiner.compute(img, feat)\n",
    "\n",
    "            # make output record\n",
    "            rec = {\n",
    "                \"original_path\": orig_path,\n",
    "                \"class\": class_name,\n",
    "            }\n",
    "            rec.update(feats)\n",
    "            rows_out.append(rec)\n",
    "\n",
    "    if not rows_out:\n",
    "        print(\"[warn] no rows produced.\")\n",
    "        return\n",
    "\n",
    "    out_df = pd.DataFrame(rows_out)\n",
    "    # Nice ordering: front-matter, raw_63 (optional), then geo/img features\n",
    "    front_cols = [\"original_path\", \"class\"]\n",
    "    raw_cols   = [c for c in out_df.columns if c.startswith(\"raw_f\")]\n",
    "    geo_cols   = sorted([c for c in out_df.columns if c.startswith(\"geo_\")])\n",
    "    img_cols   = sorted([c for c in out_df.columns if c.startswith(\"img_\")])\n",
    "\n",
    "    ordered = front_cols + raw_cols + geo_cols + img_cols\n",
    "    # keep any leftover columns (unlikely) at the end\n",
    "    ordered += [c for c in out_df.columns if c not in ordered]\n",
    "\n",
    "    out_df = out_df[ordered]\n",
    "    os.makedirs(os.path.dirname(OUT_CSV), exist_ok=True)\n",
    "    out_df.to_csv(OUT_CSV, index=False)\n",
    "    print(f\"[done] wrote {len(out_df)} rows → {OUT_CSV}\")\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2803ed",
   "metadata": {},
   "source": [
    "### Train the Knn and SVM Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6488e665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ============ CONFIG ============\n",
    "# csv_path = r\"D:\\Files\\2025_Y4_S2\\AMME5710\\Major\\combined_features.csv\"\n",
    "# k_neighbors = 5\n",
    "# cv_folds = 5\n",
    "# random_state = 42\n",
    "\n",
    "# # ============ LOAD DATA ============\n",
    "# df = pd.read_csv(csv_path).dropna()\n",
    "# X = df.drop(columns=[\"original_path\", \"class\"]).select_dtypes(include=[np.number]).values\n",
    "# y = df[\"class\"].values\n",
    "\n",
    "# # # ============ MODEL- KNN============\n",
    "# # pipe = make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors=k_neighbors))\n",
    "# # cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=random_state)\n",
    "\n",
    "# # # Out-of-fold predictions for full confusion matrix\n",
    "# # y_pred = cross_val_predict(pipe, X, y, cv=cv, n_jobs=-1)\n",
    "# # pipe.fit(X, y)\n",
    "    \n",
    "# # joblib.dump(pipe, r\"D:/5710/MajorProj/Models/knn_model.pkl\")\n",
    "\n",
    "\n",
    "# #=========== MODEL- SVM ============\n",
    "# # --- model: Standardize → SVM(RBF) ---\n",
    "# pipe = make_pipeline(\n",
    "#     StandardScaler(),\n",
    "#     SVC(kernel=\"rbf\", C=10.0, gamma=\"scale\", class_weight=None, random_state=random_state)\n",
    "# )\n",
    "\n",
    "# cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=random_state)\n",
    "# y_pred = cross_val_predict(pipe, X, y, cv=cv, n_jobs=-1)\n",
    "# pipe.fit(X, y)\n",
    "\n",
    "\n",
    "# joblib.dump(pipe, r\"D:\\Files\\2025_Y4_S2\\AMME5710\\Major\\svm_model.pkl\")\n",
    "\n",
    "# #=========== Accuracy ============\n",
    "\n",
    "# # Accuracy (overall)\n",
    "# acc = accuracy_score(y, y_pred)\n",
    "# print(f\"\\n Overall Cross-Validated Accuracy: {acc*100:.2f}%\")\n",
    "\n",
    "# # Detailed report\n",
    "# print(\"\\n--- Classification Report ---\")\n",
    "# print(classification_report(y, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "# # ============ CONFUSION MATRICES ============\n",
    "# labels = np.unique(y)\n",
    "# cm = confusion_matrix(y, y_pred, labels=labels)\n",
    "# cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "\n",
    "# # --- Plot raw confusion matrix ---\n",
    "# plt.figure(figsize=(10,8))\n",
    "# sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "#             xticklabels=labels, yticklabels=labels)\n",
    "# plt.xlabel(\"Predicted\")\n",
    "# plt.ylabel(\"True\")\n",
    "# plt.title(f\"KNN Confusion Matrix (k={k_neighbors}) — Raw Counts\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # --- Plot normalized accuracy matrix ---\n",
    "# plt.figure(figsize=(10,8))\n",
    "# sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Greens',\n",
    "#             xticklabels=labels, yticklabels=labels, cbar_kws={'label': 'Accuracy (%)'})\n",
    "# plt.xlabel(\"Predicted\")\n",
    "# plt.ylabel(\"True\")\n",
    "# plt.title(f\"KNN Normalized Confusion Matrix (Per-Class Accuracy) — k={k_neighbors}\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # Optional: show mean class accuracy\n",
    "# class_acc = np.diag(cm_norm)\n",
    "# for lbl, acc in zip(labels, class_acc):\n",
    "#     print(f\"{lbl:<8s} → {acc*100:.2f}%\")\n",
    "# print(f\"\\nMean class accuracy: {class_acc.mean()*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cdb49089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 8.24204683e-01  1.42967895e-01  1.22148879e-01]\n",
      " [ 1.40649486e+00  5.35759389e-01  1.57074153e-01]\n",
      " [ 1.11456597e+00  8.14407587e-01  1.88937306e-01]\n",
      " [ 4.25572962e-01  7.68537164e-01  2.16642261e-01]\n",
      " [ 6.22233808e-01  9.96111691e-01  1.66676398e-02]\n",
      " [ 6.00370049e-01  1.38354421e+00  1.06821604e-01]\n",
      " [ 5.15221119e-01  1.39591324e+00  1.92325294e-01]\n",
      " [ 4.78653640e-01  1.23093593e+00  2.36673325e-01]\n",
      " [ 3.89803390e-09  9.99999940e-01 -2.47683263e-10]\n",
      " [-1.18900537e-01  1.44190300e+00  9.48145390e-02]\n",
      " [-1.68710470e-01  1.35957026e+00  1.91226318e-01]\n",
      " [-9.86276492e-02  1.13225114e+00  2.37159029e-01]\n",
      " [-5.33894360e-01  9.50102508e-01  2.42256140e-03]\n",
      " [-7.00551569e-01  1.34199297e+00  8.05591941e-02]\n",
      " [-7.11256385e-01  1.29267573e+00  1.61221653e-01]\n",
      " [-5.96409976e-01  1.07347143e+00  1.96398735e-01]\n",
      " [-1.06012237e+00  8.43661964e-01  1.66676398e-02]\n",
      " [-1.19876611e+00  1.16690445e+00  6.38704970e-02]\n",
      " [-1.13527441e+00  1.17816615e+00  1.02553807e-01]\n",
      " [-9.78691697e-01  1.04321277e+00  1.21765025e-01]]\n",
      "Predicted class: E\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Michaelli\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.4.2 when using version 1.7.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\Michaelli\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator SVC from version 1.4.2 when using version 1.7.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\Michaelli\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator Pipeline from version 1.4.2 when using version 1.7.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "img = cv2.imread(\"D:/Files/2025_Y4_S2/AMME5710/Major/Root/Video/20251102/20251102/20251102_010446_722116_0.png\")\n",
    "combiner = FeatureCombiner(include_original63=False)\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "row = []\n",
    "with mp_hands.Hands(static_image_mode=True,\n",
    "                    max_num_hands=1,\n",
    "                    min_detection_confidence=0.8,model_complexity=1) as hands:\n",
    "    r = process_image_with_mediapipe(img, hands)\n",
    "    if r is None:\n",
    "        print(f\"[skip] cannot read \")\n",
    "    if r[\"n_hands\"] == 0:\n",
    "        print(f\"[no hand]\")\n",
    "    for idx, hinfo in enumerate(r[\"hands\"]):\n",
    "        row = np.array(hinfo[\"feature63\"], dtype=np.float32)\n",
    "        # Now reshape safely\n",
    "        L = row.reshape((21, 3))\n",
    "        Lc, info = normalize_hand_orientation(\n",
    "            L,\n",
    "            method=\"basis\",\n",
    "            kabsch_with_scale=True,\n",
    "            mirror_thumb=True\n",
    "        )\n",
    "        z_zoomfactor = 2.5\n",
    "        Lc[:, 0] *= z_zoomfactor\n",
    "        print(Lc)\n",
    "    \n",
    "\n",
    "FEATURE_ORDER = [\n",
    "    \"geo_angle_index_j1\", \"geo_angle_index_j2\", \"geo_angle_index_j3\",\n",
    "    \"geo_angle_middle_j1\", \"geo_angle_middle_j2\", \"geo_angle_middle_j3\",\n",
    "    \"geo_angle_pinky_j1\", \"geo_angle_pinky_j2\", \"geo_angle_pinky_j3\",\n",
    "    \"geo_angle_ring_j1\", \"geo_angle_ring_j2\", \"geo_angle_ring_j3\",\n",
    "    \"geo_angle_thumb_j1\", \"geo_angle_thumb_j2\", \"geo_angle_thumb_j3\",\n",
    "    \"geo_palm_area\",\n",
    "    \"geo_spread_idx_mid\", \"geo_spread_mid_ring\", \"geo_spread_ring_pink\",\n",
    "    \"geo_z_max\", \"geo_z_mean\", \"geo_z_min\", \"geo_z_rng\", \"geo_z_std\",\n",
    "    \"img_edge_density\", \"img_lap_var\"\n",
    "]\n",
    "\n",
    "# --- compute features as usual ---\n",
    "feats = combiner.compute(img, Lc)\n",
    "\n",
    "# --- reorder & convert ---\n",
    "feat_vec = np.array([feats.get(k, np.nan) for k in FEATURE_ORDER], dtype=np.float32).reshape(1, -1)\n",
    "\n",
    "\n",
    "svm_model = joblib.load(r\"D:/Files/2025_Y4_S2/AMME5710/Major/svm_model.pkl\")\n",
    "X_test = np.array([list(feat_vec.flatten())], dtype=np.float32)\n",
    "y_pred = svm_model.predict(X_test)\n",
    "print(f\"Predicted class: {y_pred[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
